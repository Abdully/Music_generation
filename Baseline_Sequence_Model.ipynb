{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "# Download midi library from here: http://deeplearning.net/tutorial/rnnrbm.html\n",
    "# and do sysn.path.append with it, like below.\n",
    "# Direct link: http://www.iro.umontreal.ca/~lisa/deep/midi.zip\n",
    "PATH = '/home/ubuntu/git/music-generation/'\n",
    "sys.path.append(os.path.join(PATH, 'midi'))\n",
    "\n",
    "import pretty_midi\n",
    "from midi_utils import midiread, midiwrite\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "\n",
    "def midi_filename_to_piano_roll(midi_filename):\n",
    "    \n",
    "    midi_data = midiread(midi_filename, dt=0.3)\n",
    "    \n",
    "    piano_roll = midi_data.piano_roll.transpose()\n",
    "    \n",
    "    # Binarize the pressed notes\n",
    "    piano_roll[piano_roll > 0] = 1\n",
    "    \n",
    "    return piano_roll\n",
    "\n",
    "\n",
    "def pad_piano_roll(piano_roll, max_length=132333, pad_value=0):\n",
    "    \n",
    "    # We hardcode 128 -- because we will always use only\n",
    "    # 128 pitches\n",
    "    \n",
    "    original_piano_roll_length = piano_roll.shape[1]\n",
    "    \n",
    "    padded_piano_roll = np.zeros((88, max_length))\n",
    "    padded_piano_roll[:] = pad_value\n",
    "    \n",
    "    padded_piano_roll[:, :original_piano_roll_length] = piano_roll\n",
    "\n",
    "    return padded_piano_roll\n",
    "\n",
    "\n",
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, midi_folder_path, longest_sequence_length=1491):\n",
    "        \n",
    "        self.midi_folder_path = midi_folder_path\n",
    "        \n",
    "        midi_filenames = os.listdir(midi_folder_path)\n",
    "        \n",
    "        self.longest_sequence_length = longest_sequence_length\n",
    "        \n",
    "        midi_full_filenames = map(lambda filename: os.path.join(midi_folder_path, filename),\n",
    "                                  midi_filenames)\n",
    "        \n",
    "        self.midi_full_filenames = list(midi_full_filenames)\n",
    "        \n",
    "        if longest_sequence_length is None:\n",
    "            \n",
    "            self.update_the_max_length()\n",
    "    \n",
    "    \n",
    "    def update_the_max_length(self):\n",
    "        \"\"\"Recomputes the longest sequence constant of the dataset.\n",
    "\n",
    "        Reads all the midi files from the midi folder and finds the max\n",
    "        length.\n",
    "        \"\"\"\n",
    "        \n",
    "        sequences_lengths = map(lambda filename: midi_filename_to_piano_roll(filename).shape[1],\n",
    "                                self.midi_full_filenames)\n",
    "        \n",
    "        max_length = max(sequences_lengths)\n",
    "        \n",
    "        self.longest_sequence_length = max_length\n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.midi_full_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        midi_full_filename = self.midi_full_filenames[index]\n",
    "        \n",
    "        piano_roll = midi_filename_to_piano_roll(midi_full_filename)\n",
    "        \n",
    "        # -1 because we will shift it\n",
    "        sequence_length = piano_roll.shape[1] - 1\n",
    "        \n",
    "        # Shifted by one time step\n",
    "        input_sequence = piano_roll[:, :-1]\n",
    "        ground_truth_sequence = piano_roll[:, 1:]\n",
    "                \n",
    "        # pad sequence so that all of them have the same lenght\n",
    "        # Otherwise the batching won't work\n",
    "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length)\n",
    "        \n",
    "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,\n",
    "                                                      max_length=self.longest_sequence_length,\n",
    "                                                      pad_value=-100)\n",
    "                \n",
    "        input_sequence_padded = input_sequence_padded.transpose()\n",
    "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
    "        \n",
    "        return (torch.FloatTensor(input_sequence_padded),\n",
    "                torch.LongTensor(ground_truth_sequence_padded),\n",
    "                torch.LongTensor([sequence_length]) )\n",
    "\n",
    "    \n",
    "def post_process_sequence_batch(batch_tuple):\n",
    "    \n",
    "    input_sequences, output_sequences, lengths = batch_tuple\n",
    "    \n",
    "    splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
    "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
    "    splitted_lengths_batch = lengths.split(split_size=1)\n",
    "\n",
    "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
    "                               splitted_output_sequence_batch,\n",
    "                               splitted_lengths_batch)\n",
    "\n",
    "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
    "                                         key=lambda p: int(p[2]),\n",
    "                                         reverse=True)\n",
    "\n",
    "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
    "\n",
    "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
    "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
    "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
    "    \n",
    "    # Here we trim overall data matrix using the size of the longest sequence\n",
    "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0], :]\n",
    "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0], :]\n",
    "    \n",
    "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
    "    \n",
    "    # pytorch's api for rnns wants lenghts to be list of ints\n",
    "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
    "    lengths_batch_sorted_list = list(map(lambda x: int(x), lengths_batch_sorted_list))\n",
    "    \n",
    "    return input_sequence_batch_transposed, output_sequence_batch_sorted, lengths_batch_sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset('Nottingham_data/Nottingham/train/')\n",
    "\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=120,\n",
    "                                              shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "valset = NotesGenerationDataset('Nottingham_data/Nottingham/valid/', longest_sequence_length=None)\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size=30, \n",
    "                                            shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
    "        \n",
    "        batch_size = input_sequences.shape[1]\n",
    "\n",
    "        notes_encoded = self.notes_encoder(input_sequences)\n",
    "        \n",
    "        # Here we run rnns only on non-padded regions of the batch\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(notes_encoded, input_sequences_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        \n",
    "        logits = self.logits_fc(outputs)\n",
    "                \n",
    "        logits = logits.transpose(0, 1).contiguous()\n",
    "        \n",
    "        neg_logits = (1 - logits)\n",
    "        \n",
    "        # Since the BCE loss doesn't support masking, we use the crossentropy\n",
    "        binary_logits = torch.stack((logits, neg_logits), dim=3).contiguous()\n",
    "        \n",
    "        logits_flatten = binary_logits.view(-1, 2)\n",
    "        \n",
    "        return logits_flatten, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "rnn = rnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "criterion_val = nn.CrossEntropyLoss(size_average=False).cuda()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "\n",
    "    full_val_loss = 0.0\n",
    "    overall_sequence_length = 0.0\n",
    "\n",
    "    for batch in valset_loader:\n",
    "\n",
    "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "        output_sequences_batch_var =  output_sequences_batch.contiguous().view(-1).cuda()\n",
    "\n",
    "        input_sequences_batch_var = input_sequences_batch.cuda()\n",
    "\n",
    "        logits, _ = rnn(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "        loss = criterion_val(logits, output_sequences_batch_var)\n",
    "\n",
    "        full_val_loss += loss.item()\n",
    "        overall_sequence_length += sum(sequences_lengths)\n",
    "\n",
    "    return full_val_loss / (overall_sequence_length * 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "\tTRAIN LOSS: 0.0926\n",
      "\tVAL LOSS: 0.0926\n",
      "EPOCH 1\n",
      "\tTRAIN LOSS: 0.0926\n",
      "\tVAL LOSS: 0.0919\n",
      "EPOCH 2\n",
      "\tTRAIN LOSS: 0.0920\n",
      "\tVAL LOSS: 0.0916\n",
      "EPOCH 3\n",
      "\tTRAIN LOSS: 0.0924\n",
      "\tVAL LOSS: 0.0914\n",
      "EPOCH 4\n",
      "\tTRAIN LOSS: 0.0915\n",
      "\tVAL LOSS: 0.0909\n",
      "EPOCH 5\n",
      "\tTRAIN LOSS: 0.0915\n",
      "\tVAL LOSS: 0.0907\n",
      "EPOCH 6\n",
      "\tTRAIN LOSS: 0.0909\n",
      "\tVAL LOSS: 0.0902\n",
      "EPOCH 7\n",
      "\tTRAIN LOSS: 0.0899\n",
      "\tVAL LOSS: 0.0899\n",
      "EPOCH 8\n",
      "\tTRAIN LOSS: 0.0898\n",
      "\tVAL LOSS: 0.0889\n",
      "EPOCH 9\n",
      "\tTRAIN LOSS: 0.0892\n",
      "\tVAL LOSS: 0.0884\n",
      "EPOCH 10\n",
      "\tTRAIN LOSS: 0.0878\n",
      "\tVAL LOSS: 0.0873\n",
      "EPOCH 11\n",
      "\tTRAIN LOSS: 0.0877\n",
      "\tVAL LOSS: 0.0865\n",
      "EPOCH 12\n",
      "\tTRAIN LOSS: 0.0856\n",
      "\tVAL LOSS: 0.0861\n",
      "EPOCH 13\n",
      "\tTRAIN LOSS: 0.0856\n",
      "\tVAL LOSS: 0.0848\n",
      "EPOCH 14\n",
      "\tTRAIN LOSS: 0.0844\n",
      "\tVAL LOSS: 0.0840\n",
      "EPOCH 15\n",
      "\tTRAIN LOSS: 0.0835\n",
      "\tVAL LOSS: 0.0830\n",
      "EPOCH 16\n",
      "\tTRAIN LOSS: 0.0828\n",
      "\tVAL LOSS: 0.0823\n",
      "EPOCH 17\n",
      "\tTRAIN LOSS: 0.0818\n",
      "\tVAL LOSS: 0.0815\n",
      "EPOCH 18\n",
      "\tTRAIN LOSS: 0.0811\n",
      "\tVAL LOSS: 0.0808\n",
      "EPOCH 19\n",
      "\tTRAIN LOSS: 0.0805\n",
      "\tVAL LOSS: 0.0798\n",
      "EPOCH 20\n",
      "\tTRAIN LOSS: 0.0794\n",
      "\tVAL LOSS: 0.0791\n",
      "EPOCH 21\n",
      "\tTRAIN LOSS: 0.0786\n",
      "\tVAL LOSS: 0.0783\n",
      "EPOCH 22\n",
      "\tTRAIN LOSS: 0.0776\n",
      "\tVAL LOSS: 0.0775\n",
      "EPOCH 23\n",
      "\tTRAIN LOSS: 0.0773\n",
      "\tVAL LOSS: 0.0766\n",
      "EPOCH 24\n",
      "\tTRAIN LOSS: 0.0762\n",
      "\tVAL LOSS: 0.0758\n",
      "EPOCH 25\n",
      "\tTRAIN LOSS: 0.0754\n",
      "\tVAL LOSS: 0.0751\n",
      "EPOCH 26\n",
      "\tTRAIN LOSS: 0.0749\n",
      "\tVAL LOSS: 0.0743\n",
      "EPOCH 27\n",
      "\tTRAIN LOSS: 0.0737\n",
      "\tVAL LOSS: 0.0736\n",
      "EPOCH 28\n",
      "\tTRAIN LOSS: 0.0731\n",
      "\tVAL LOSS: 0.0729\n",
      "EPOCH 29\n",
      "\tTRAIN LOSS: 0.0722\n",
      "\tVAL LOSS: 0.0725\n",
      "EPOCH 30\n",
      "\tTRAIN LOSS: 0.0720\n",
      "\tVAL LOSS: 0.0718\n",
      "EPOCH 31\n",
      "\tTRAIN LOSS: 0.0707\n",
      "\tVAL LOSS: 0.0711\n",
      "EPOCH 32\n",
      "\tTRAIN LOSS: 0.0707\n",
      "\tVAL LOSS: 0.0709\n",
      "EPOCH 33\n",
      "\tTRAIN LOSS: 0.0704\n",
      "\tVAL LOSS: 0.0704\n",
      "EPOCH 34\n",
      "\tTRAIN LOSS: 0.0701\n",
      "\tVAL LOSS: 0.0698\n",
      "EPOCH 35\n",
      "\tTRAIN LOSS: 0.0697\n",
      "\tVAL LOSS: 0.0695\n",
      "EPOCH 36\n",
      "\tTRAIN LOSS: 0.0697\n",
      "\tVAL LOSS: 0.0690\n",
      "EPOCH 37\n",
      "\tTRAIN LOSS: 0.0684\n",
      "\tVAL LOSS: 0.0691\n",
      "EPOCH 38\n",
      "\tTRAIN LOSS: 0.0688\n",
      "\tVAL LOSS: 0.0685\n",
      "EPOCH 39\n",
      "\tTRAIN LOSS: 0.0679\n",
      "\tVAL LOSS: 0.0682\n",
      "EPOCH 40\n",
      "\tTRAIN LOSS: 0.0682\n",
      "\tVAL LOSS: 0.0679\n",
      "EPOCH 41\n",
      "\tTRAIN LOSS: 0.0677\n",
      "\tVAL LOSS: 0.0675\n",
      "EPOCH 42\n",
      "\tTRAIN LOSS: 0.0675\n",
      "\tVAL LOSS: 0.0673\n",
      "EPOCH 43\n",
      "\tTRAIN LOSS: 0.0668\n",
      "\tVAL LOSS: 0.0669\n",
      "EPOCH 44\n",
      "\tTRAIN LOSS: 0.0665\n",
      "\tVAL LOSS: 0.0666\n",
      "EPOCH 45\n",
      "\tTRAIN LOSS: 0.0665\n",
      "\tVAL LOSS: 0.0662\n",
      "EPOCH 46\n",
      "\tTRAIN LOSS: 0.0661\n",
      "\tVAL LOSS: 0.0659\n",
      "EPOCH 47\n",
      "\tTRAIN LOSS: 0.0658\n",
      "\tVAL LOSS: 0.0659\n",
      "EPOCH 48\n",
      "\tTRAIN LOSS: 0.0658\n",
      "\tVAL LOSS: 0.0655\n",
      "EPOCH 49\n",
      "\tTRAIN LOSS: 0.0651\n",
      "\tVAL LOSS: 0.0651\n",
      "EPOCH 50\n",
      "\tTRAIN LOSS: 0.0653\n",
      "\tVAL LOSS: 0.0648\n",
      "EPOCH 51\n",
      "\tTRAIN LOSS: 0.0652\n",
      "\tVAL LOSS: 0.0646\n",
      "EPOCH 52\n",
      "\tTRAIN LOSS: 0.0642\n",
      "\tVAL LOSS: 0.0645\n",
      "EPOCH 53\n",
      "\tTRAIN LOSS: 0.0645\n",
      "\tVAL LOSS: 0.0641\n",
      "EPOCH 54\n",
      "\tTRAIN LOSS: 0.0645\n",
      "\tVAL LOSS: 0.0639\n",
      "EPOCH 55\n",
      "\tTRAIN LOSS: 0.0643\n",
      "\tVAL LOSS: 0.0638\n",
      "EPOCH 56\n",
      "\tTRAIN LOSS: 0.0636\n",
      "\tVAL LOSS: 0.0636\n",
      "EPOCH 57\n",
      "\tTRAIN LOSS: 0.0638\n",
      "\tVAL LOSS: 0.0635\n",
      "EPOCH 58\n",
      "\tTRAIN LOSS: 0.0637\n",
      "\tVAL LOSS: 0.0632\n",
      "EPOCH 59\n",
      "\tTRAIN LOSS: 0.0631\n",
      "\tVAL LOSS: 0.0629\n",
      "EPOCH 60\n",
      "\tTRAIN LOSS: 0.0628\n",
      "\tVAL LOSS: 0.0629\n",
      "EPOCH 61\n",
      "\tTRAIN LOSS: 0.0630\n",
      "\tVAL LOSS: 0.0626\n",
      "EPOCH 62\n",
      "\tTRAIN LOSS: 0.0634\n",
      "\tVAL LOSS: 0.0628\n",
      "EPOCH 63\n",
      "\tTRAIN LOSS: 0.0627\n",
      "\tVAL LOSS: 0.0627\n",
      "EPOCH 64\n",
      "\tTRAIN LOSS: 0.0626\n",
      "\tVAL LOSS: 0.0623\n",
      "EPOCH 65\n",
      "\tTRAIN LOSS: 0.0620\n",
      "\tVAL LOSS: 0.0621\n",
      "EPOCH 66\n",
      "\tTRAIN LOSS: 0.0620\n",
      "\tVAL LOSS: 0.0618\n",
      "EPOCH 67\n",
      "\tTRAIN LOSS: 0.0621\n",
      "\tVAL LOSS: 0.0616\n",
      "EPOCH 68\n",
      "\tTRAIN LOSS: 0.0617\n",
      "\tVAL LOSS: 0.0616\n",
      "EPOCH 69\n",
      "\tTRAIN LOSS: 0.0614\n",
      "\tVAL LOSS: 0.0613\n",
      "EPOCH 70\n",
      "\tTRAIN LOSS: 0.0616\n",
      "\tVAL LOSS: 0.0613\n",
      "EPOCH 71\n",
      "\tTRAIN LOSS: 0.0612\n",
      "\tVAL LOSS: 0.0610\n",
      "EPOCH 72\n",
      "\tTRAIN LOSS: 0.0613\n",
      "\tVAL LOSS: 0.0609\n",
      "EPOCH 73\n",
      "\tTRAIN LOSS: 0.0609\n",
      "\tVAL LOSS: 0.0607\n",
      "EPOCH 74\n",
      "\tTRAIN LOSS: 0.0609\n",
      "\tVAL LOSS: 0.0606\n",
      "EPOCH 75\n",
      "\tTRAIN LOSS: 0.0610\n",
      "\tVAL LOSS: 0.0605\n",
      "EPOCH 76\n",
      "\tTRAIN LOSS: 0.0605\n",
      "\tVAL LOSS: 0.0605\n",
      "EPOCH 77\n",
      "\tTRAIN LOSS: 0.0606\n",
      "\tVAL LOSS: 0.0603\n",
      "EPOCH 78\n",
      "\tTRAIN LOSS: 0.0603\n",
      "\tVAL LOSS: 0.0602\n",
      "EPOCH 79\n",
      "\tTRAIN LOSS: 0.0605\n",
      "\tVAL LOSS: 0.0601\n",
      "EPOCH 80\n",
      "\tTRAIN LOSS: 0.0601\n",
      "\tVAL LOSS: 0.0599\n",
      "EPOCH 81\n",
      "\tTRAIN LOSS: 0.0600\n",
      "\tVAL LOSS: 0.0599\n",
      "EPOCH 82\n",
      "\tTRAIN LOSS: 0.0598\n",
      "\tVAL LOSS: 0.0596\n",
      "EPOCH 83\n",
      "\tTRAIN LOSS: 0.0599\n",
      "\tVAL LOSS: 0.0596\n",
      "EPOCH 84\n",
      "\tTRAIN LOSS: 0.0597\n",
      "\tVAL LOSS: 0.0595\n",
      "EPOCH 85\n",
      "\tTRAIN LOSS: 0.0598\n",
      "\tVAL LOSS: 0.0594\n",
      "EPOCH 86\n",
      "\tTRAIN LOSS: 0.0595\n",
      "\tVAL LOSS: 0.0594\n",
      "EPOCH 87\n",
      "\tTRAIN LOSS: 0.0594\n",
      "\tVAL LOSS: 0.0592\n",
      "EPOCH 88\n",
      "\tTRAIN LOSS: 0.0590\n",
      "\tVAL LOSS: 0.0591\n",
      "EPOCH 89\n",
      "\tTRAIN LOSS: 0.0590\n",
      "\tVAL LOSS: 0.0591\n",
      "EPOCH 90\n",
      "\tTRAIN LOSS: 0.0590\n",
      "\tVAL LOSS: 0.0589\n",
      "EPOCH 91\n",
      "\tTRAIN LOSS: 0.0588\n",
      "\tVAL LOSS: 0.0590\n",
      "EPOCH 92\n",
      "\tTRAIN LOSS: 0.0589\n",
      "\tVAL LOSS: 0.0588\n",
      "EPOCH 93\n",
      "\tTRAIN LOSS: 0.0586\n",
      "\tVAL LOSS: 0.0586\n",
      "EPOCH 94\n",
      "\tTRAIN LOSS: 0.0588\n",
      "\tVAL LOSS: 0.0584\n",
      "EPOCH 95\n",
      "\tTRAIN LOSS: 0.0582\n",
      "\tVAL LOSS: 0.0583\n",
      "EPOCH 96\n",
      "\tTRAIN LOSS: 0.0583\n",
      "\tVAL LOSS: 0.0582\n",
      "EPOCH 97\n",
      "\tTRAIN LOSS: 0.0581\n",
      "\tVAL LOSS: 0.0581\n",
      "EPOCH 98\n",
      "\tTRAIN LOSS: 0.0581\n",
      "\tVAL LOSS: 0.0579\n",
      "EPOCH 99\n",
      "\tTRAIN LOSS: 0.0582\n",
      "\tVAL LOSS: 0.0579\n",
      "EPOCH 100\n",
      "\tTRAIN LOSS: 0.0576\n",
      "\tVAL LOSS: 0.0578\n",
      "EPOCH 101\n",
      "\tTRAIN LOSS: 0.0579\n",
      "\tVAL LOSS: 0.0579\n",
      "EPOCH 102\n",
      "\tTRAIN LOSS: 0.0576\n",
      "\tVAL LOSS: 0.0577\n",
      "EPOCH 103\n",
      "\tTRAIN LOSS: 0.0574\n",
      "\tVAL LOSS: 0.0577\n",
      "EPOCH 104\n",
      "\tTRAIN LOSS: 0.0576\n",
      "\tVAL LOSS: 0.0574\n",
      "EPOCH 105\n",
      "\tTRAIN LOSS: 0.0575\n",
      "\tVAL LOSS: 0.0574\n",
      "EPOCH 106\n",
      "\tTRAIN LOSS: 0.0572\n",
      "\tVAL LOSS: 0.0575\n",
      "EPOCH 107\n",
      "\tTRAIN LOSS: 0.0574\n",
      "\tVAL LOSS: 0.0573\n",
      "EPOCH 108\n",
      "\tTRAIN LOSS: 0.0575\n",
      "\tVAL LOSS: 0.0571\n",
      "EPOCH 109\n",
      "\tTRAIN LOSS: 0.0570\n",
      "\tVAL LOSS: 0.0569\n",
      "EPOCH 110\n",
      "\tTRAIN LOSS: 0.0569\n",
      "\tVAL LOSS: 0.0569\n",
      "EPOCH 111\n",
      "\tTRAIN LOSS: 0.0565\n",
      "\tVAL LOSS: 0.0568\n",
      "EPOCH 112\n",
      "\tTRAIN LOSS: 0.0568\n",
      "\tVAL LOSS: 0.0566\n",
      "EPOCH 113\n",
      "\tTRAIN LOSS: 0.0564\n",
      "\tVAL LOSS: 0.0565\n",
      "EPOCH 114\n",
      "\tTRAIN LOSS: 0.0565\n",
      "\tVAL LOSS: 0.0564\n",
      "EPOCH 115\n",
      "\tTRAIN LOSS: 0.0564\n",
      "\tVAL LOSS: 0.0564\n",
      "EPOCH 116\n",
      "\tTRAIN LOSS: 0.0562\n",
      "\tVAL LOSS: 0.0562\n",
      "EPOCH 117\n",
      "\tTRAIN LOSS: 0.0559\n",
      "\tVAL LOSS: 0.0561\n",
      "EPOCH 118\n",
      "\tTRAIN LOSS: 0.0558\n",
      "\tVAL LOSS: 0.0560\n",
      "EPOCH 119\n",
      "\tTRAIN LOSS: 0.0557\n",
      "\tVAL LOSS: 0.0559\n"
     ]
    }
   ],
   "source": [
    "clip = 1.0\n",
    "epochs_number = 120#000000\n",
    "sample_history = []\n",
    "best_val_loss = float(\"inf\")\n",
    "loss_list = list()\n",
    "val_list = list()\n",
    "\n",
    "for epoch_number in range(epochs_number):\n",
    "\n",
    "    current_train_loss = 0\n",
    "    for batch in trainset_loader:\n",
    "\n",
    "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "        output_sequences_batch_var =  output_sequences_batch.contiguous().view(-1).cuda()\n",
    "        \n",
    "        input_sequences_batch_var = input_sequences_batch.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, _ = rnn(input_sequences_batch_var, sequences_lengths)\n",
    "        \n",
    "        loss = criterion(logits, output_sequences_batch_var)\n",
    "        loss_list.append( loss.item() )\n",
    "        current_train_loss += loss_list[-1]\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    current_train_loss /= len(trainset_loader)\n",
    "\n",
    "    current_val_loss = validate()\n",
    "    val_list.append(current_val_loss)\n",
    "    \n",
    "    if current_val_loss < best_val_loss:\n",
    "        \n",
    "        torch.save(rnn.state_dict(), 'music_rnn.pth')\n",
    "        best_val_loss = current_val_loss\n",
    "    print('EPOCH %d' % epoch_number)\n",
    "    print('\\tTRAIN LOSS: %.4f' % current_train_loss)\n",
    "    print('\\tVAL LOSS: %.4f' % current_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "rnn = rnn.cuda()\n",
    "rnn.load_state_dict(torch.load('models/music_rnn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(sample_length=4, temperature=1, starting_sequence=None):\n",
    "\n",
    "    if starting_sequence is None:\n",
    "                \n",
    "        current_sequence_input = torch.zeros(1, 1, 88)\n",
    "        current_sequence_input[0, 0, 40] = 1\n",
    "        current_sequence_input[0, 0, 50] = 0\n",
    "        current_sequence_input[0, 0, 56] = 0\n",
    "        current_sequence_input = Variable(current_sequence_input.cuda())\n",
    "    else:\n",
    "        current_sequence_input = starting_sequence\n",
    "    final_output_sequence = [current_sequence_input.data.squeeze(1)]\n",
    "    \n",
    "    hidden = None\n",
    "\n",
    "    for i in range(sample_length):\n",
    "\n",
    "        output, hidden = rnn(current_sequence_input, [1], hidden)\n",
    "\n",
    "        probabilities = nn.functional.softmax(output.div(temperature), dim=1)\n",
    "\n",
    "        current_sequence_input = torch.multinomial(probabilities.data, 1).squeeze().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        current_sequence_input = Variable(current_sequence_input.float())\n",
    "\n",
    "        final_output_sequence.append(current_sequence_input.data.squeeze(1))\n",
    "\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = NotesGenerationDataset('Nottingham_data/Nottingham/test/', longest_sequence_length=None)\n",
    "\n",
    "testset_loader = torch.utils.data.DataLoader(testset, batch_size=1, \n",
    "                                            shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(testset_loader))\n",
    "post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "output_sequences_batch_var =  output_sequences_batch.contiguous().view(-1).cuda()\n",
    "\n",
    "input_sequences_batch_var = input_sequences_batch.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_orig = batch[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_orig = sample_orig.reshape((1495, 88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "midiwrite('test0_orig.mid', sample_orig, dt=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f008f3bc5f8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample_orig.transpose()[:,:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='test0_orig.mid' target='_blank'>test0_orig.mid</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/git/music-generation/test0_orig.mid"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.FileLink('test0_orig.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0095064438>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sample = sample_from_piano_rnn(sample_length=100, temperature=0.7, \n",
    "                               starting_sequence=input_sequences_batch_var).transpose()\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 260)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "midiwrite('test0.mid', sample.transpose(), dt=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='test0.mid' target='_blank'>test0.mid</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/git/music-generation/test0.mid"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.FileLink('test0.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
